# Pesticide_pdf_checker
This code based on langgraph takes a pdf as input, parse it through OCR, extract the relevant information and check wether the analysis found on the pdf are compliant with EU regulations.

## Installation

Clone the repository, create a virtual environment, and install the required packages:

```bash
git clone https://github.com/antoniomoricca/Pesticide_pdf_checker.git
pip install -r requirements.txt
```
Before running the code, make sure to set up the following .env file in the project root with
- your OPENAI API key:
```
- OPENAI_API_KEY=your_api_key_here
```
- Tesseract OCR: download and install Tesseract from https://github.com/tesseract-ocr/tesseract
 and set the path to the tesserect .ext.
```
- TESSERACT_CMD=C:/Program Files/Tesseract-OCR/tesseract.exe
```


## Usage

You can use the tool in two ways:

1. **Via CLI**: process a PDF file directly from the command line:

```bash
python cli.py --pdf "path/to/your/file.pdf"
```

2. **Via Demo Notebook**: open Demo_Notebook.ipynb to explore the main functions imported from src/. The notebook generates the LangGraph graph and lets you visualize how the system state evolves step by step.

## Project Structure

- `data/` → contains the PDF files used as input.  
- `src/` → source code with all the main functions.  
- `tests/` → pytest tests to verify that the LangGraph and functions work correctly.  
- `main.py` → main entry point for running the code.  
- `cli.py` → command-line interface to process PDFs.  
- `Demo_Notebook.ipynb` → notebook demo showing how to use the functions.


## How it Works

The project processes PDF files to extract information, feeds this information to a Large Language Model (LLM) to generate a JSON, and then enriches the JSON via API calls to obtain the EU pesticide limits for the specific products.  

At the core of the architecture, **LangGraph was used to orchestrate the workflow**. This design was chosen because it is robust, production-ready, and scalable: multiple tools can be added, and a human-in-the-loop step can be incorporated if needed.  It is also possible to use an LLM as a router to decide which tools to call or whether to take intermediate steps back. However, giving more control to the LLM can reduce robustness, so trade-offs should be carefully evaluated.  

The various tools in the LangGraph are connected in a chain, performing each step of the workflow in sequence. These tools include:

1. **PDF Processing**  
   PDF processing is implemented using OCR, since the PDFs analyzed are saved as images. The functions take a PDF, convert it into images to improve quality, and then parse the text using `pytesseract` OCR.  
   The process is repeated multiple times with different DPI (dots per inch) settings. DPI is a measure of image resolution—the higher the DPI, the more detailed the image. TThe quality of the PDFs, font size, and layout are not known in advance.  After running OCR at various DPI levels, the extracted text containing the most characters is selected. This heuristic assumes that more extracted characters correspond to a better reading of the PDF content.  
Potential improvements could be made in this step to find the optimal configuration more efficiently, for example by automatically estimating the best DPI or using more advanced image preprocessing.

2. **LLM Processing**  
The text extracted from the PDFs is fed into a Large Language Model (OPENAI), which is responsible for extracting the key information: product, analyzed substance, and analysis values.  
An LLM was chosen for its flexibility in retrieving and translating information from PDFs. The prompt was structured as a **one-shot example**, without using real data (since only three PDFs were available, it would have been too easy).  
Using **few-shot learning** with real data could significantly improve the robustness and accuracy of the LLM output.
The output of the LLM is a **json**, easy and robust to use for the next steps

3. **JSON Enriching**  
Once the JSON output is generated by the LLM, it is enriched using EU product and pesticide databases, accessed via APIs.  
From these databases, `product_id` and `substance_id` are retrieved. These identifiers are then used to query the EU MRL (Maximum Residue Level) database, obtaining the legal limit value for each molecule associated with each product.  
All this information is added to the JSON, making it complete and ready for further analysis or reporting.

4. **Final Report Printing**  
At this stage, each molecule under analysis is compared against its respective MRL (Maximum Residue Level).  
A report is generated showing whether the analysis results are compliant with EU regulations. This allows users to quickly check compliance for each product and substance.




